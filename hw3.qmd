---
title: "Homework 3"
author: "Kiran Charangat {style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: pdf
# format: pdf
editor: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

::: {.callout-important style="font-size: 0.8em;"}
Please read the instructions carefully before submitting your
assignment.

1.  This assignment requires you to only upload a `PDF` file on Canvas
2.  Don't collapse any code cells before submitting.
3.  Remember to make sure all your code output is rendered properly
    before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter
before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine
Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset
from the UCI Machine Learning Repository. The dataset consists of red
and white *vinho verde* wine samples, from the north of Portugal. The
goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{R}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
```

```{R}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

```{R}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```

## <br><br><br><br>

## Question 1

::: callout-tip
## 50 points

Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in
data frames `df1` and `df2`.

```{R}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"


df1 <- read_delim(url1, delim = ";", show_col_types = FALSE)
df2 <- read_delim(url2, delim = ";", show_col_types = FALSE)
```

```{R}
colnames(df1)
```

```{R}
head(df1)

```

------------------------------------------------------------------------

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1.  Combine the two data frames into a single data frame `df`, adding a
    new column called `type` to indicate whether each row corresponds to
    white or red wine.
2.  Rename the columns of `df` to replace spaces with underscores
3.  Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
4.  Convert the `type` column to a factor
5.  Remove rows (if any) with missing values.

```{R}
df <- bind_rows(df1 %>% mutate(type = "white"), 
                df2 %>% mutate(type = "red")) %>%
      mutate(type = as.factor(type)) %>% 
      drop_na() %>%
      rename_with(~ gsub(" ", "_", .x)) %>%
      select(-fixed_acidity, -free_sulfur_dioxide)
               
```

```{R}
colnames(df)
```

```{R}
dim(df)
```

Your output to `R dim(df)` should be

```         
[1] 6497   11
```

------------------------------------------------------------------------

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the
the difference in means (with the equal variance assumption)

1.  Using `df` compute the mean of `quality` for red and white wine
    separately, and then store the difference in means as a variable
    called `diff_mean`.

2.  Compute the pooled sample variance and store the value as a variable
    called `sp_squared`.

3.  Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and
    store its value in a variable called `t1`.

```{R}
diff_mean <-df %>%
            group_by(type) %>%
            summarise(mean_quality = mean(quality)) %>%
            summarise(diff_mean = diff(mean_quality)) %>%
            pull(diff_mean)
diff_mean

```

```{R}
sp_squared <- df %>% 
              group_by(type) %>%
              summarise(n = n(),
                        var_quality = var(quality)) %>%
              summarise(sp_squared = ((n[1] - 1) * var_quality[1] + (n[2] - 1) *                           var_quality[2]) / (sum(n) - 2)) %>%
              pull(sp_squared)

sp_squared

              
```

```{R}
t1 <-  df %>%
      summarise(t1 = diff_mean / sqrt(sp_squared * (1/sum(df$type == "white") + 1                 /sum(df$type == "red")))) %>%
      pull(t1)

t1
```

------------------------------------------------------------------------

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to
perform a two-sample $t$-Test without having to compute the pooled
variance and difference in means.

Perform a two-sample t-test to compare the quality of white and red
wines using the `t.test()` function with the setting `var.equal=TRUE`.
Store the t-statistic in `t2`.

```{R}
t_test <- t.test(quality ~ type, data = df, var.equal = TRUE) 
t_test

```

```{R}
t2 <- t_test$statistic
t2
```

------------------------------------------------------------------------

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the
`lm()` function, and extract the $t$-statistic for the `type`
coefficient from the model summary. Store this $t$-statistic in `t3`.

```{R}
fit <- lm(quality ~ type, data = df) 
fit
```

```{R}
t3 <- summary(fit)$coefficients[2, "t value"]
t3
```

------------------------------------------------------------------------

###### 1.6 (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can
you conclude from this? Why?

```{R}
t_statistics <- c(t1, t2, t3)
t_statistics
```

```{R}
# These t-statitistic values indicate a significant difference in wine quality
# between red and white wine. The negative sign in the second value (t2) suggests
# a reversal in direction of the effect. These results, however, show that the
# wine type significantly impacts quality, but it is important to understand the
# direction of the effect.
```

<br><br><br><br> <br><br><br><br> ---

## Question 2

::: callout-tip
## 25 points

Collinearity
:::

------------------------------------------------------------------------

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response
variable `quality`. Use the `broom::tidy()` function to print a summary
of the fitted model. What can we conclude from the model summary?

```{R}
broom::tidy(fit)
```

```{R}
# The coefficient for 'typewhite' indicates the estimated change in quality score when comparing white and red wine. The positive coefficent means that white wine has the higher quality score then red wine on average with that being an increase of around 0.242 points. The effect is pretty significant as shown by the very small p-value and high t-statstic. The intercept represents the estimated quality score for the baseline category of the wine type when all the other predicators are zero. The fact that this value is pretty different then zero (5.636) suggests a pretty high baseline quality score. Overall, the model suggests that wine type is a significant predictor of wine quality, with white wines predicted to have a higher quality score than red wines
```

------------------------------------------------------------------------

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only
`citric_acid` as the predictor, and another with only
`total_sulfur_dioxide` as the predictor. In both models, use `quality`
as the response variable. How does your model summary compare to the
summary from the previous question?

```{R}
model_citric <- lm(quality ~ citric_acid, data = df)
```

```{R}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
```

------------------------------------------------------------------------

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using
`corrplot()`

```{R}
library(corrplot)
df %>% 
  select_if(is.numeric) %>% 
  cor() %>%                 
  corrplot(method = "circle") 
```

------------------------------------------------------------------------

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the
full model using `vif()` function. What can we conclude from this?

```{R}
vif(lm(quality ~ ., data = df))
```

```{R}
# The predictor density has a VIF of over 9, and type has a VIF of close to 7, which suggests that these variables have a strong linear relationship with other predictor variables in the model. Therefore, the VIF for density and type indicates that they may be collinear with one or more other predictors in the model, which means that the effects of these individual predictors on the response variable quality may not be estimated as precisely as predicted.
```

<br><br><br><br> <br><br><br><br> ---

## Question 3

::: callout-tip
## 40 points

Variable selection
:::

------------------------------------------------------------------------

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the
starting model. Store the final formula in an object called
`backward_formula` using the built-in `formula()` function in R

```{R}
full_model <- lm(quality ~ ., data = df)
backward_model <- step(full_model, direction = "backward")
backward_formula <- formula(backward_model)
backward_formula
```

------------------------------------------------------------------------

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the
starting model. Store the final formula in an object called
`forward_formula` using the built-in `formula()` function in R

```{R}
null_model <- lm(quality ~ 1, data = df)
forward_model <- step(null_model, direction = "forward", scope = formula(full_model))
forward_formula <- formula(forward_model)
forward_formula
```

------------------------------------------------------------------------

###### 3.3 (10 points)

1.  Create a `y` vector that contains the response variable (`quality`)
    from the `df` dataframe.

2.  Create a design matrix `X` for the `full_model` object using the
    `make_model_matrix()` function provided in the Appendix.

3.  Then, use the `cv.glmnet()` function to perform LASSO and Ridge
    regression with `X` and `y`.

```{R}
y <- df$quality
X <- make_model_matrix(formula(full_model))
lasso_model <- cv.glmnet(X, y, alpha = 1)
ridge_model <- cv.glmnet(X, y, alpha = 0)
```

Create side-by-side plots of the ridge and LASSO regression results.
Interpret your main findings.

```{R}
par(mfrow=c(1, 2))
plot(lasso_model, main="LASSO Regression")
plot(ridge_model, main="Ridge Regression")
```

```{R}
# From these models, it be concluded that both models benefit from regularization, as indicated by the fact that the MSE is minimized for non-zero penalties. These plots are used to select an appropriate level of regularization to balance model complexity and prediction error. They suggest that both LASSO and Ridge regression have specific regularization strengths that minimize cross-validated prediction error for this dataset.
```

------------------------------------------------------------------------

###### 3.4 (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se`
value? What are the variables selected by LASSO?

```{R}
lasso_coef_at_1se <- coef(lasso_model, s = "lambda.1se")
lasso_coef_at_1se
```

```{R}
# The variables selected by LASSO are volatile_acidity with a coefficient of approximately -0.19, residual_sugar with a coefficient of approximately 0.044, sulphates with a coefficient of approximately 0.058, and alcohol with a coefficient of approximately 0.368
```

Store the variable names with non-zero coefficients in `lasso_vars`, and
create a formula object called `lasso_formula` using the
`make_formula()` function provided in the Appendix.

```{R}
lasso_coef_vector <- as.numeric(lasso_coef_at_1se)
names(lasso_coef_vector) <- rownames(lasso_coef_at_1se)
lasso_vars <- names(lasso_coef_vector)[-1][lasso_coef_vector[-1] != 0]
lasso_formula <- make_formula(lasso_vars)
lasso_formula
```

------------------------------------------------------------------------

###### 3.5 (5 points)

Print the coefficient values for ridge regression at the `lambda.1se`
value? What are the variables selected here?

```{R}
ridge_coef_at_1se <- coef(ridge_model, s = "lambda.1se")
ridge_coef_at_1se 
```

Store the variable names with non-zero coefficients in `ridge_vars`, and
create a formula object called `ridge_formula` using the
`make_formula()` function provided in the Appendix.

```{R}
ridge_vars <- rownames(ridge_coef_at_1se)[-1] 
ridge_formula <- make_formula(ridge_vars)
print(ridge_formula)
```

------------------------------------------------------------------------

###### 3.6 (10 points)

What is the difference between stepwise selection, LASSO and ridge based
on you analyses above?

```{R}
# The stepwise selection would have given me a subset of predictors based on a criterion like AIC, without any coefficient shrinkage.
# The LASSO regression results showed which variables were most influential by keeping their coefficients away from zero while others were shrunk to zero.
# The Ridge regression results would have included all predictors but with their influence reduced, as indicated by the smaller coefficients compared to the original unregularized model. Overall, stepwise selection is a purely statistical approach to model simplification, LASSO is a shrinkage method that can also perform variable selection, and Ridge is a shrinkage method that aims to improve model performance without variable selection.
```

<br><br><br><br> <br><br><br><br> ---

## Question 4

::: callout-tip
## 70 points

Variable selection
:::

------------------------------------------------------------------------

###### 4.1 (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the
covariates. How many different models can we create using any subset of
these $10$ coavriates as possible predictors? Justify your answer.

```{R}
# We can create 2^10 - 1 which is 1023 different models. Each predictor can either be included in or excluded from a model, which is a binary choice, leading to 2^n combinations and we subtract one because we do not consider a model with no predictors. 

```

------------------------------------------------------------------------

4.2 (20 points)

Store the names of the predictor variables (all columns except quality)
in an object called x_vars.

x_vars \<- colnames(df %\>% select(-quality))

```{R}
x_vars <- colnames(df %>% select(-quality))
```

Use:

-   the `combn()` function (built-in R function) and
-   the `make_formula()` (provided in the Appendix)

to **generate all possible linear regression formulas** using the
variables in `x_vars`. This is most optimally achieved using the `map()`
function from the `purrr` package.

```{R}
formulas <- map(
  1:length(x_vars),
  \(x){
    vars <- combn(x_vars, x, simplify = FALSE) 
    map(vars, \(combo) make_formula(combo))
  }
) %>% unlist(recursive = FALSE)
```

If your code is right the following command should return something
along the lines of:

```{R}
sample(formulas, 4) %>% as.character()
```

``` r
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

------------------------------------------------------------------------

###### 4.3 (10 points)

Use `map()` and `lm()` to fit a linear regression model to each formula
in `formulas`, using `df` as the data source. Use `broom::glance()` to
extract the model summary statistics, and bind them together into a
single tibble of summaries using the `bind_rows()` function from
`dplyr`.

```{R}
models <- map(formulas, ~lm(.x, data = df)) 
summaries <- map(models, ~broom::glance(.x)) 
all_summaries <- bind_rows(summaries)
all_summaries
```

------------------------------------------------------------------------

###### 4.4 (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to
identify the formula with the ***highest*** adjusted R-squared value.

```{R}
best_model_index <- which.max(all_summaries$adj.r.squared)
rsq_formula <- formulas[[best_model_index]]
rsq_formula

```

Store resulting formula as a variable called `rsq_formula`.

------------------------------------------------------------------------

###### 4.5 (5 points)

Extract the `AIC` values from `summaries` and use them to identify the
formula with the ***lowest*** AIC value.

```{R}
lowest_aic_index <- which.min(all_summaries$AIC)
aic_formula <- formulas[[lowest_aic_index]]
aic_formula 
```

Store resulting formula as a variable called `aic_formula`.

------------------------------------------------------------------------

###### 4.6 (15 points)

Combine all formulas shortlisted into a single vector called
`final_formulas`.

```{R}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)

```

-   Are `aic_formula` and `rsq_formula` the same? How do they differ
    from the formulas shortlisted in question 3?

    -   The aic_formula and rsq_formula are almost similar as the
        rsq_formula includes total_sulfur_dioxide and aic_formula does
        not. As for how they differ from the formulas shortlisted in
        question 3, The aic_formula and rsq_formula represent models
        selected based on optimizing specific statistical criteria. The
        formulas from question 3 however, like the ridge_formula and
        lasso_formula apply regularization techniques and the forward
        and backward formulas use step wise procedures.

-   Which of these is more reliable? Why?

    -   For prediction purposes and generalization, aic_formula and
        lasso_formula might be good options. For explanatory purposes,
        rsq_formula might be good like when explaining the variance in
        response is the primary goal.

-   If we had a dataset with $10,000$ columns, which of these methods
    would you consider for your analyses? Why?

    -   I would choose the LASSO and Ridge regression for this analysis
        due to its effectiveness in handling multi-collinearity and its
        ability to prevent over fitting through regularization.

------------------------------------------------------------------------

###### 4.7 (10 points)

Use `map()` and `glance()` to extract the
`sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model
obtained from `final_formulas`. Bind them together into a single data
frame `summary_table`. Summarize your main findings.

```{R}
summary_table <- map(
  final_formulas, 
  \(x) {
    model <- lm(x, data = df)
    broom::glance(model) %>%
      select(sigma, adj.r.squared, AIC, df = df.residual, p.value = p.value)
  }
) %>% bind_rows()

summary_table %>% knitr::kable()
```

::: {.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br> <br><br><br><br> ---

# Appendix

#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x`
and outputs a `formula` object with `quality` as the response variable
and the columns of `x` as the covariates.

```{R}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and
outputs a **rescaled** model matrix `X` in a format amenable for
`glmnet()`

```{R}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```

::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
